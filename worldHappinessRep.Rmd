---
title: "Project 3"
author: "Anthony Drake"
date: "April 22, 2019"
output: pdf_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(broom)
library(tidyr)
library(caret)
setwd("C:/Users/Anthony/Documents/DATA220")
happy15<-read.csv("2015.csv")
happy16<-read.csv("2016.csv")
happy17<-read.csv("2017.csv")
coastline<-read.csv("coastline.csv") ## made this in DATA210
```

## Introduction

These datasets were created to report all the nations' happiness rank based off things such as their economy, health, freedom and trust in government. All the values are numerical, except for the region column, which shows which region of the world a nation is located. 

After closely examining the data, there are no missing values. However, there are 0's placed in some of the data entries but they are acting as weights, so those will be left alone.

```{r na, include=FALSE}
any(is.na(happy15))
any(is.na(happy16))
any(is.na(happy17))
```

## Preparing the Data

Before rbinding all 3 datasets into one, I need to remove the columns that are not in common with all 3 (except for Region). I used the R console to do so until all 3 datasets had the same variables. After doing this I realized that the happy17 columns weren't in the same order as happy15 and happy16, so I reordered them before rbinding.

One more thing I did was combine allthree dataset into one.

```{r pressure, echo=FALSE}

happy15$Region<-NULL
happy16$Region<-NULL
happy17$Region<-NULL
happy15$Standard.Error<-NULL
happy16$Lower.Confidence.Interval<-NULL
happy16$Upper.Confidence.Interval<-NULL
happy17$Whisker.high<-NULL
happy17$Whisker.low<-NULL

happy17<-happy17[,c(1,2,3,4,5,6,7,9,8,10)]

happy15<-cbind(happy15,rep(2015,nrow(happy15)))
happy16<-cbind(happy16,rep(2016,nrow(happy16)))
happy17<-cbind(happy17,rep(2017,nrow(happy17)))


names(happy15)[11]<-"Year"
names(happy16)[11]<-"Year"
names(happy17)[11]<-"Year"

happy15$`rep(2015, nrow(happy15))`<-NULL
happy16$`rep(2016, nrow(happy16))`<-NULL
happy17$`rep(2017, nrow(happy17))`<-NULL

```

## Combining the Datasets and Making Categorical Variables

 I took the liberty of creating 3 categorical variables based on the freedom score, happiness score and GDP. The freedom score cuts the values in the middle and the countries are either labeled "Oppressed" or "Free". The happiness categorical variable  does the same as above but cuts its continuous variable counterpart into 3 pieces and labels them "Unhappy", "Moderate" or "Content". The GDP which also has 3 pieces, "Poor", "Moderate" and "Wealthy". And finally, a categorical variable for the life expectancy column. This will categorize them in 3 groups determining whether life expectancy has a "good", "fair" or "poor" correlations to happiness.

```{r}
happy<-rbind(happy15,happy16,happy17)
happy$Year<-factor(happy$Year)

happy$y<-NULL
happy$happyCat<-cut(happy$Happiness.Score,breaks = c(2.69,4.32,5.96,7.59),labels= c("Unhappy","Moderate","Content"))
happy$freeCat<-cut(happy$Freedom,breaks = c(0,0.334865,0.66973),labels= c("Oppressed","Free"))
happy$GDPCat<-cut(happy$Economy..GDP.per.Capita.,breaks = c(-0.00187,0.624,1.25,1.87),labels = c("Poor","Moderate","Wealthy"))
happy$lifeCat<-cut(happy$Health..Life.Expectancy.,breaks = c(-0.00103,0.342,0.683,1.03),labels = c("Poor","Fair","Good"))

```


## Building the Logistical Regression Model

Let's train out model to predict the freedom level of a nation based on their happiness level and GDP. The first model below shows an AIC of 507.21 which, for this model, is not very adequate. There isn't very much statistical significance for neither years nor life expectancy so I will be dropping them and retraining the model.

```{r train, echo=TRUE}
train <- happy %>%
  select(freeCat,Year,happyCat,GDPCat,lifeCat) %>%
  na.omit()

mod<-glm(freeCat~.,train,family = binomial)

summary(mod)
```


Below is the retrained model. The AIC is just a hair less than my previous model, so I will be choosing this model. We also need to keep in mind that the outcome is in log odds, not probability, so I will have to change this with line line of code so it's easier to interpret.

```{r}
train2 <- happy %>%
  select(freeCat,happyCat,GDPCat) %>%
  na.omit()

mod2<-glm(freeCat~.,train2,family = binomial)

summary(mod2)

exp(coef(mod2))

summary(mod2$residuals)

sd(mod2$residuals)
```

After changing the coefficients from log odds to odds using an exponent, I noticed that happyCatContent and happyCatModerate have an odds that is greater than 1. This could most likely stem from the large standad deviation, considering it is nearly 1000 times greater than the mean and more than double that of that median.

## Interpreting Coefficients

```{r}
exp(coef(mod2))
```

Let's disregard the happyCatContent and happyCatModerate values for these interpretations.

For every nation in the world that has a wealthy GDP, the odds of the country being considered "free" drops by 35.75%. For a country that has an adequate GDP then the odds drop by 50.62%. 

Now let's try interpreting results. Let's take a specific case: 0 nations with a moderate GDP and 2 nations with a wealthy GDP. The odds that all of them are categorized as free nations is about 45.42%.

For 3 nations with a moderate GDP and 0 nations with a wealthy GDP the odds are much less, 13.25%.

For 2 moderate GDP nations and 3 rich GDP nations? 7.12%

```{r}
x<- coefficients(mod2)[1] + 0* coefficients(mod2)[4] + 2* coefficients(mod2)[5]
exp(x)

y<- coefficients(mod2)[1] + 3* coefficients(mod2)[4] + 0* coefficients(mod2)[5]
exp(y)

z<- coefficients(mod2)[1] + 2* coefficients(mod2)[4] + 3* coefficients(mod2)[5]
exp(z)

linear<- augment(mod2) %>%
  select(lfit = .fitted)
isFree <- mod2 %>%
  augment(type.predict = "response") %>%
  mutate(yhat = .fitted,
         odds_hat = yhat/(1-yhat))
isFree<- cbind(isFree,linear)
isFree<- isFree %>%
  select(yhat,odds_hat,lfit)

ggplot(isFree,aes(x=lfit,y=odds_hat)) + geom_point()
```

Above is a logistic distribution plot showing the odds of 1 occurring on an exponential distribution. So if the lfit increases then so does the odds_hat.

# K Nearest Neighbor

Now let's do a knn model to identify the best k value for our dataset. First, let's build a training model to create structure around our data. Because their isn't any abstraction in knn, we just simply "plug and chug". I've decided to create a new dataset for knn but the only change I'm making to it is making the happyCat variable 2 levels instead of 3, since the cl variable for my training model should only be 2 levels.

```{r knn}
library(class)

happy$happyCat<-cut(happy$Happiness.Score,breaks = c(2.693,5.14,7.587),labels= c("Unhappy","Content"))
happyKnn<-happy[,c(2:9,12)]

set.seed(8)
tidx <- sample(nrow(happyKnn),round(.6*nrow(happyKnn)))
train <- happyKnn[tidx,-9]
test <- happyKnn[-tidx,-9]
cl <- happyKnn[tidx,9]
orig <- happyKnn[-tidx,9]
pred <- knn(train,test,cl,k=3,prob=TRUE)

table(orig,pred)
```

The confusion matrix above tells us the how well the features compare to their neighbors. 

## Visualizing the Results

```{r plot}
ggplot(data = test,aes(Freedom, Happiness.Score,col= pred, shape=orig)) + geom_point()
```

```{r accuracy}
x<-table(orig,pred)
acc <-sum(diag(x))/sum(x)
paste0(round(acc*100,2),"%")


```

The accuracy for the knn model is 96.28%. We shouldn't, however, take this as gospel because it doesn't really penalize like the kappa statistic does.

```{r recall and precision}
y<-matrix(x,nrow = 2,byrow = TRUE)
row.names(y) <-c("Unhappy","Content")
colnames(y) <- c("Unhappy","Content")
y

paste0(round(y[2,2]/sum(y[2,])*100,2),"%")

paste0(round(y[2,2]/sum(y[,2])*100,2),"%")

```
The recall of the matrix is 95.37%, meaning that that percentage of unhappy people has been correctly identified.

The second percentage shows the precision of the model. So it was able to predict nations that were "content" 98.1% of the time.



```{r kappa}
nmarg <- (sum(x[1,])*sum(x[,1]))/sum(x)
pmarg <- (sum(x[2,])*sum(x[,2]))/sum(x)
xacc <- (nmarg+pmarg)/sum(x)
kappa <- (acc-xacc)/(1-xacc)
paste0(round(kappa*100,2),"%")
```

This is a better representation of the accuracy of the knn model. Although it is still in the 90s, this percentage shouldn't be something to be skeptical about because kappa takes into account the possibility of the agreement occurring by chance and not just when it actually happens. 

## Finding Best K value

```{r}
set.seed(3033)
tidx <- createDataPartition(happyKnn$happyCat, p= 0.5, list=FALSE)
train <- happyKnn[tidx,]
test <- happyKnn[-tidx,]
trctrl <- trainControl(method = "repeatedcv", number = 10,
repeats = 3)
knn_fit <- train(happyCat ~., data = train, method = "knn",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
knn_fit
```

